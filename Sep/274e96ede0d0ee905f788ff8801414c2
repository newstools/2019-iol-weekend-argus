Experts in ethics, privacy and bias in AI spoke at the AI Expo Africa conference held in Cape Town this week. Highlights included an AI art and music challenge, and French company Rhoban who showed off their soccer-playing robots. The robots are also the RoboWorld Cup champions. Mushambi Mutuma, chief executive at tech company Altivex said AI was controlled by the people who input the data. “Last year Google’s facial recognition software kept inputting African-Americans as gorillas. We have to have new voices and a bit more diversity because the data will never come back accurate and that’s what we’ve tried to solve from bias,” he said. Andrew Quixley works with data science and AI at IBM and said the company also struggled with bias in their AI programmes. “We were one of the companies that were caught out with AI that was not performing well with female faces and darker skin faces. “We have a bias detector and we have a product to find out why bias exists in a model,” he said. But Quixley said humans might not be ready to eliminate bias from AI in any case, adding: “Which person should die in a car accident in a car that was being driven autonomously? Out of 40 million human inputs the bias is clear. “We’d rather save someone who is law abiding, a female, has a higher income than the person in the other car. We need to say, do we want to be the same as humans or smarter than humans? We’re going to have to accept imperfection of some kind.” The price to pay for living in the digital age is having your data floating about in cyberspace susceptible to hacking or surveillance. This week, the City of Joburg announced plans to roll out a massive 15000-camera network that will capture the movements of residents. “You can anticipate the crimes you want to legislate for but it isn’t perfect. There’s a man in the UK that’s suing the court because he claims that the facial recognition software isn’t good enough for him to be convicted,” said Quixley. However, in a survey conducted by Genesys, it found that more than half of the employers that participated in the multi-country survey said their companies “do not currently have a written policy on the ethical use of AI or bots”. In the same survey 21% said they were concerned that their companies could use AI in an unethical manner. Rachel Alexander, founder of Omina Technologies, said companies should be ethical in how they use their AI because they could run into potential liable issues. Alexander said if an AI bot was being used to evaluate workers’ performance there could be potential problems that arose from a faulty one. “It could be that your algorithm is firing all the fat people in your company and you can’t explain your decision. Companies need to think about how they’re going to open up and explain how the decision-making is being done.” She said following guidelines for ethical AI was important and cited the EU’s guidelines as a good example. “I was fairly involved with EU guidelines and they put together a coalition with academics, start-up companies and lawyers as well and tried to reach a consensus about what the ethical guidelines and principles should be. “The EU is talking about augmented intelligence. They had a doctor diagnose cancer in a patient and AI diagnose cancer. Then the doctor and the AI diagnosed the patient together and the results came out better so they want it to be machine and human working together,” said Alexander. Many companies are pursuing the implementation of AI models in their business to streamline workflows and help employees eliminate mundane tasks from their work to use their time more productively. Weekend Argus